# Acoustic-Based Drone Detection System ğŸšğŸ“¡

![Status](https://img.shields.io/badge/Status-Completed-success)
![Python](https://img.shields.io/badge/Python-3.11-blue)
![Hardware](https://img.shields.io/badge/Hardware-Raspberry_Pi_5-red)

**Real-Time UAV Identification and Localization using Edge AI**

---

## ğŸ“– Table of Contents

- [About the Project](#-about-the-project)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Hardware Requirements](#-hardware-requirements)
- [Tech Stack](#-tech-stack)
- [Dataset](#-dataset)
- [Installation](#-installation)
- [Usage](#-usage)
- [Results & Performance](#-results--performance)
- [Team & Acknowledgements](#-team--acknowledgements)
- [License](#-license)

---

## ğŸ“ About the Project

Unmanned Aerial Vehicles (UAVs) pose increasing security risks to critical infrastructure and personal privacy. Traditional detection techniques such as radar and vision-based systems often struggle with small consumer drones, cluttered environments, or low-light conditions.

**Acoustic Drone Detection System** designed for real-time airspace monitoring. It uses a **Raspberry Pi 5** and a **Seeed Studio ReSpeaker 4-Mic Array** to capture the unique acoustic signatures of drone motors. A lightweight **Random Forest** machine-learning model classifies incoming audio and estimates the **Direction of Arrival (DOA)**, enabling rapid localization. The entire system runs **offline at the edge**, ensuring privacy and low latency.

---

## âœ¨ Key Features

- **ğŸŒ² Edge AI Core:** Optimized Random Forest classifier achieving **~96% accuracy** with sub-350 ms inference latency.
- **ğŸ§­ Real-Time Localization:** Estimates sound source direction (0Â°â€“360Â°) using Time Difference of Arrival (TDOA) from the 4-microphone array.
- **ğŸ”‡ Intelligent Noise Filtering:** Differentiates drones from wind, traffic, and human speech using MFCC, spectral, and temporal features.
- **ğŸ–¥ï¸ Tactical Dashboard:** Standalone Python GUI with radar-style visualization and clear detection status (Red/Green).
- **ğŸ”’ Privacy-First:** Fully offline operation with no cloud dependency.
- **âš¡ Automatic Gain Control (AGC):** Dynamically adjusts microphone sensitivity to detect distant drones.

---

## ğŸ— System Architecture

The system follows a modular, real-time processing pipeline:

1. **Audio Capture:** Raw multi-channel audio input from the ReSpeaker 4-Mic Array
2. **Preprocessing:** Noise reduction, framing, normalization, and digital AGC
3. **Feature Extraction:** MFCCs, Spectral Contrast, Chroma, and Zero-Crossing Rate using `librosa`
4. **Inference Engine:** Random Forest model classifies audio as **Drone** or **Noise**
5. **Localization:** DOA algorithm estimates the angle of arrival when a drone is detected
6. **Visualization:** Radar dashboard updates with target position and confidence

---

## ğŸ›  Hardware Requirements

- **Single Board Computer:** Raspberry Pi 5 (8 GB RAM recommended)
- **Microphone Array:** Seeed Studio ReSpeaker 4-Mic Array (USB)
- **Power Supply:** USB-C PD 27 W
- **Cooling:** Raspberry Pi Active Cooler
- **Display:** HDMI monitor
- **Storage:** High-speed microSD card (32 GB or higher)

---

## ğŸ’» Tech Stack

- **Programming Language:** Python 3.11
- **Machine Learning:** Scikit-learn, Joblib
- **Audio Processing:** Librosa, NumPy, PyAudio, SciPy
- **Hardware Interface:** Seeed Voicecard Drivers
- **Visualization & GUI:** Matplotlib (Animation API), Tkinter

---

## ğŸ“Š Dataset

A custom dataset was created using real-world recordings of commercial quadcopters and diverse environmental noise samples.

ğŸ”— **Dataset:**  
https://www.kaggle.com/datasets/gautamdhawan55/merged-drone

**Dataset Structure**

- `drone/` â€“ 148+ UAV motor sound samples
- `noise/` â€“ 125+ ambient noise samples

---

## âš™ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/acoustic-drone-detection.git
cd acoustic-drone-detection
```

### 2. Install System Dependencies (Raspberry Pi)

```bash
sudo apt update
sudo apt install python3-pyaudio portaudio19-dev libatlas-base-dev
```

### 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### Phase 1: Model Training (Optional)

1. Place the dataset folder inside the project directory
2. Open `training.ipynb`
3. Run all cells to preprocess audio and train the model
4. Generated files:
   - `drone_brain_v2.pkl`
   - `feature_scaler.pkl`

---

### Phase 2: Real-Time Deployment

```bash
python3 main.py
```

The radar dashboard will launch and highlight detected drones in **red** with direction locking.

---

## ğŸ“ˆ Results & Performance

- **Classification Accuracy:** 96.32%
- **Inference Latency:** < 350 ms per audio chunk
- **Detection Range:** ~10 meters
- **Localization Accuracy:** Â±15Â° error
- **Thermal Stability:** CPU temperature < 65 Â°C

---

## ğŸ‘¥ Team & Acknowledgements

**Capstone Project (CPG-179)**  
**Thapar Institute of Engineering & Technology, Patiala**

**Team Members**

- Miet Pamecha (102203012)
- Gautam Dhawan (102203061)
- Lipsita Devgan (102203408)
- Tamanna Bajaj (102203413)
- Devansh Dhir (102203449)

**Faculty Mentor**

- **Dr. Sharad Saxena**  
  Associate Professor, Department of Computer Science & Engineering

---
